{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c312b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "############## Entire dataset clustering based on similar sentences\n",
    "!pip install sentence_transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import scipy.cluster.hierarchy as hcluster\n",
    "import pandas as pd\n",
    "\n",
    "embedder = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "stop =['Properties','Bank','Company','LLC', 'Family','Companies','Inc.','Co.','Ltd.','LP', 'Group']\n",
    "data= pd.read_csv(\"C:\\\\Users\\\\KD129201\\\\data_ideal.csv\")\n",
    "#data= pd.read_csv(\"C:\\\\Users\\\\KD129201\\\\Ideal_Companies_Reduced.csv\")\n",
    "df = pd.DataFrame(data) \n",
    "df['Name'].apply(lambda x: [item for item in x if item not in stop])\n",
    "\n",
    "corpus = df['Name'].tolist()\n",
    "#corpus1= df['Website'].to_list()\n",
    "#corpus2=corpus1.dropna()\n",
    "corpus_embeddings = embedder.encode(corpus)\n",
    "#corpus_embeddings1 = embedder.encode(corpus2)\n",
    "\n",
    "# Perform hierarchical clustering\n",
    "cosine_threshold = 0.88\n",
    "cluster_assignment = hcluster.fclusterdata(\n",
    "    corpus_embeddings, \n",
    "    1-cosine_threshold, \n",
    "    criterion=\"distance\", \n",
    "    metric=\"cosine\", \n",
    "    method=\"average\"\n",
    "    )\n",
    "\n",
    "#cosine_threshold_web = 0.100\n",
    "#cluster_assignment_web = hcluster.fclusterdata(\n",
    "#    corpus_embeddings1, \n",
    "#    1-cosine_threshold_web, \n",
    "#    criterion=\"distance\", \n",
    "#    metric=\"cosine\", \n",
    "#    method=\"average\"\n",
    "#    )\n",
    "\n",
    "num_clusters = len(set(cluster_assignment))\n",
    "clustered_sentences = [[] for i in range(num_clusters)]\n",
    "for sentence_id, cluster_id in enumerate(cluster_assignment):\n",
    "    clustered_sentences[cluster_id-1].append(corpus[sentence_id])\n",
    "\n",
    "\n",
    "\n",
    "#prediction = pd.DataFrame(columns=['i', 'cluster'])\n",
    "data=[]\n",
    "#for i, cluster in enumerate(clustered_sentences):\n",
    "   # prediction = prediction.append(pd.DataFrame(data={'i': i, 'cluster': cluster}, index = [0]), ignore_index = True)\n",
    "#    print(\"Cluster \", i+1)\n",
    "#    print(cluster)\n",
    "#    print(\"\")\n",
    "  #  prediction = prediction.append(pd.DataFrame(data={'i': i, 'cluster': cluster}, index = [0]), ignore_index = True)\n",
    "cluster1 = [cluster for cluster in enumerate(clustered_sentences)]\n",
    "cluster1 = [tup[1] for tup in cluster1]\n",
    "data = {'cluster': cluster1}\n",
    "dff= pd.DataFrame(data, columns=['cluster'])\n",
    "print(dff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a2827a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The script MUST contain a function named azureml_main\n",
    "# which is the entry point for this module.\n",
    "\n",
    "# imports up here can be used to\n",
    "\n",
    "import os\n",
    "os.system('pip install sentence_transformers')\n",
    "#get_python().system('pip install sentence_transformers')\n",
    "\n",
    "import scipy.cluster.hierarchy as hcluster\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# The entry point function MUST have two input arguments.\n",
    "# If the input port is not connected, the corresponding\n",
    "# dataframe argument will be None.\n",
    "#   Param<dataframe1>: a pandas.DataFrame\n",
    "#   Param<dataframe2>: a pandas.DataFrame\"\n",
    "def azureml_main(dataframe1=\"C:\\\\Users\\\\KD129201\\\\Ideal_Companies_Reduced.csv\" , dataframe2=None):\n",
    "    #print(f'Input pandas.DataFrame #1: {dataframe1}')\n",
    "    #data= pd.read_csv(\"Ideal_Companies_Reduced\")\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    embedder = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "    df = pd.DataFrame(dataframe1) \n",
    "    corpus = df['Name'].tolist()\n",
    "    #corpus1= df['Website'].to_list()\n",
    "    #corpus2=corpus1.dropna()\n",
    "    corpus_embeddings = embedder.encode(corpus)\n",
    "    #corpus_embeddings1 = embedder.encode(corpus2)\n",
    "\n",
    "    # Perform hierarchical clustering\n",
    "    cosine_threshold = 0.80\n",
    "    cluster_assignment = hcluster.fclusterdata(\n",
    "        corpus_embeddings, \n",
    "        1-cosine_threshold, \n",
    "        criterion=\"distance\", \n",
    "        metric=\"cosine\", \n",
    "        method=\"average\"\n",
    "        )\n",
    "\n",
    "    #cosine_threshold_web = 0.100\n",
    "    #cluster_assignment_web = hcluster.fclusterdata(\n",
    "    #    corpus_embeddings1, \n",
    "    #    1-cosine_threshold_web, \n",
    "    #    criterion=\"distance\", \n",
    "    #    metric=\"cosine\", \n",
    "    #    method=\"average\"\n",
    "    #    )\n",
    "\n",
    "    num_clusters = len(set(cluster_assignment))\n",
    "    clustered_sentences = [[] for i in range(num_clusters)]\n",
    "    for sentence_id, cluster_id in enumerate(cluster_assignment):\n",
    "        clustered_sentences[cluster_id-1].append(corpus[sentence_id])\n",
    "\n",
    "\n",
    "\n",
    "    #prediction = pd.DataFrame(columns=['i', 'cluster'])\n",
    "    data=[]\n",
    "    #for i, cluster in enumerate(clustered_sentences):\n",
    "    # prediction = prediction.append(pd.DataFrame(data={'i': i, 'cluster': cluster}, index = [0]), ignore_index = True)\n",
    "    #    print(\"Cluster \", i+1)\n",
    "    #    print(cluster)\n",
    "    #    print(\"\")\n",
    "    #  prediction = prediction.append(pd.DataFrame(data={'i': i, 'cluster': cluster}, index = [0]), ignore_index = True)\n",
    "    cluster1 = [cluster for cluster in enumerate(clustered_sentences)]\n",
    "    data = {'cluster': cluster1}\n",
    "    dff= pd.DataFrame(data, columns=['cluster'])\n",
    " \n",
    "\n",
    "    # If a zip file is connected to the third input port,\n",
    "    # it is unzipped under \"./Script Bundle\". This directory is added\n",
    "    # to sys.path. Therefore, if your zip file contains a Python file\n",
    "    # mymodule.py you can import it using:\n",
    "    # import mymodule\n",
    "\n",
    "    # Return value must be of a sequence of pandas.DataFrame\n",
    "    # E.g.\n",
    "    #   -  Single return value: return dataframe1,\n",
    "    #   -  Two return values: return dataframe1, dataframe2\n",
    "    return dff,\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a6a108",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435f958d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69464a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install sentence_transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import scipy.cluster.hierarchy as hcluster\n",
    "import pandas as pd\n",
    "\n",
    "embedder = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "stop =['Properties','Bank','Company','LLC', 'Family','Companies','Inc.','Co.','Ltd.','LP', 'Group']\n",
    "#data= pd.read_csv(\"C:\\\\Users\\\\KD129201\\\\demo.csv\")\n",
    "data= pd.read_csv(\"C:\\\\Users\\\\KD129201\\\\Ideal_Companies_Reduced.csv\")\n",
    "df1 = pd.DataFrame(data) \n",
    "\n",
    "df['Name']=df1['Name'].apply(lambda words: ' '.join(word.lower() for word in words.split() if word not in stop))\n",
    "corpus = df['Name'].tolist()\n",
    "#corpus1= df['Website'].to_list()\n",
    "#corpus2=corpus1.dropna()\n",
    "corpus_embeddings = embedder.encode(corpus)\n",
    "#corpus_embeddings1 = embedder.encode(corpus2)\n",
    "\n",
    "# Perform hierarchical clustering\n",
    "cosine_threshold = 0.88\n",
    "cluster_assignment = hcluster.fclusterdata(\n",
    "    corpus_embeddings, \n",
    "    1-cosine_threshold, \n",
    "    criterion=\"distance\", \n",
    "    metric=\"cosine\", \n",
    "    method=\"average\"\n",
    "    )\n",
    "\n",
    "#cosine_threshold_web = 0.100\n",
    "#cluster_assignment_web = hcluster.fclusterdata(\n",
    "#    corpus_embeddings1, \n",
    "#    1-cosine_threshold_web, \n",
    "#    criterion=\"distance\", \n",
    "#    metric=\"cosine\", \n",
    "#    method=\"average\"\n",
    "#    )\n",
    "\n",
    "num_clusters = len(set(cluster_assignment))\n",
    "clustered_sentences = [[] for i in range(num_clusters)]\n",
    "for sentence_id, cluster_id in enumerate(cluster_assignment):\n",
    "    clustered_sentences[cluster_id-1].append(corpus[sentence_id])\n",
    "\n",
    "\n",
    "\n",
    "#prediction = pd.DataFrame(columns=['i', 'cluster'])\n",
    "data=[]\n",
    "#for i, cluster in enumerate(clustered_sentences):\n",
    "   # prediction = prediction.append(pd.DataFrame(data={'i': i, 'cluster': cluster}, index = [0]), ignore_index = True)\n",
    "#    print(\"Cluster \", i+1)\n",
    "#    print(cluster)\n",
    "#    print(\"\")\n",
    "  #  prediction = prediction.append(pd.DataFrame(data={'i': i, 'cluster': cluster}, index = [0]), ignore_index = True)\n",
    "cluster1 = [cluster for cluster in enumerate(clustered_sentences)]\n",
    "cluster1 = [tup[1] for tup in cluster1]\n",
    "data = {'cluster': cluster1}\n",
    "dff= pd.DataFrame(data, columns=['cluster'])\n",
    "print(dff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6778ff36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence_transformers in c:\\users\\kd129201\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: torchvision in c:\\users\\kd129201\\anaconda3\\lib\\site-packages (from sentence_transformers) (0.13.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\kd129201\\anaconda3\\lib\\site-packages (from sentence_transformers) (1.21.5)\n",
      "Requirement already satisfied: tqdm in c:\\users\\kd129201\\anaconda3\\lib\\site-packages (from sentence_transformers) (4.64.0)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\kd129201\\anaconda3\\lib\\site-packages (from sentence_transformers) (0.1.97)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in c:\\users\\kd129201\\anaconda3\\lib\\site-packages (from sentence_transformers) (4.21.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\kd129201\\anaconda3\\lib\\site-packages (from sentence_transformers) (1.7.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\kd129201\\anaconda3\\lib\\site-packages (from sentence_transformers) (1.0.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in c:\\users\\kd129201\\anaconda3\\lib\\site-packages (from sentence_transformers) (0.8.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\kd129201\\anaconda3\\lib\\site-packages (from sentence_transformers) (3.7)\n",
      "Requirement already satisfied: torch>=1.6.0 in c:\\users\\kd129201\\anaconda3\\lib\\site-packages (from sentence_transformers) (1.12.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\kd129201\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.1.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\kd129201\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\kd129201\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.6.0)\n",
      "Requirement already satisfied: requests in c:\\users\\kd129201\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.27.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\kd129201\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\kd129201\\anaconda3\\lib\\site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence_transformers) (3.0.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\kd129201\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2022.3.15)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in c:\\users\\kd129201\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.12.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\kd129201\\anaconda3\\lib\\site-packages (from tqdm->sentence_transformers) (0.4.4)\n",
      "Requirement already satisfied: click in c:\\users\\kd129201\\anaconda3\\lib\\site-packages (from nltk->sentence_transformers) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\kd129201\\anaconda3\\lib\\site-packages (from nltk->sentence_transformers) (1.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\kd129201\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\kd129201\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kd129201\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kd129201\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2021.10.8)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\kd129201\\anaconda3\\lib\\site-packages (from scikit-learn->sentence_transformers) (2.2.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\kd129201\\anaconda3\\lib\\site-packages (from torchvision->sentence_transformers) (9.0.1)\n",
      "    Id                                             Name  \\\n",
      "0  634                                       Hsu Family   \n",
      "1  635                                   Kuerbis Family   \n",
      "2  636                                     Hwang Family   \n",
      "3  637                                     Korth Family   \n",
      "4  638  Central Valley Coalition for Affordable Housing   \n",
      "5  642                               Macquarie Mortgage   \n",
      "6  644                  Chandler Management Corporation   \n",
      "7  648                             Anastasi Development   \n",
      "8  649                              J.H. Snyder Company   \n",
      "\n",
      "                    Website  \n",
      "0                       NaN  \n",
      "1                       NaN  \n",
      "2                       NaN  \n",
      "3                       NaN  \n",
      "4                       NaN  \n",
      "5           www.mcp-llc.com  \n",
      "6  www.chandlerpartners.com  \n",
      "7   http://www.anastasi.com  \n",
      "8          www.jhsnyder.net  \n"
     ]
    }
   ],
   "source": [
    "!pip install sentence_transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import scipy.cluster.hierarchy as hcluster\n",
    "import pandas as pd\n",
    "\n",
    "embedder = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "stop =['Properties','Bank','Company','LLC', 'Family','Companies','Inc.','Co.','Ltd.','LP', 'Group','inc','Inc']\n",
    "data= pd.read_csv(\"C:\\\\Users\\\\KD129201\\\\data_demo.csv\")\n",
    "#data= pd.read_csv(\"C:\\\\Users\\\\KD129201\\\\Ideal_Companies_Reduced.csv\")\n",
    "df1 = pd.DataFrame(data) \n",
    "print(df1)\n",
    "df=df1.copy()\n",
    "#df['Name']=df1['Name'].apply(lambda words: ' '.join(word.lower() for word in words.split() if word not in stop))\n",
    "corpus = df['Name'].tolist()\n",
    "#corpus1= df['Website'].to_list()\n",
    "#corpus2=corpus1.dropna()\n",
    "corpus_embeddings = embedder.encode(corpus)\n",
    "#corpus_embeddings1 = embedder.encode(corpus2)\n",
    "\n",
    "# Perform hierarchical clustering\n",
    "cosine_threshold = 0.88\n",
    "cluster_assignment = hcluster.fclusterdata(\n",
    "    corpus_embeddings, \n",
    "    1-cosine_threshold, \n",
    "    criterion=\"distance\", \n",
    "    metric=\"cosine\", \n",
    "    method=\"average\"\n",
    "    )\n",
    "\n",
    "num_clusters = len(set(cluster_assignment))\n",
    "clustered_sentences = [[] for i in range(num_clusters)]\n",
    "for sentence_id, cluster_id in enumerate(cluster_assignment):\n",
    "    clustered_sentences[cluster_id-1].append(corpus[sentence_id])\n",
    "\n",
    "\n",
    "\n",
    "#prediction = pd.DataFrame(columns=['i', 'cluster'])\n",
    "data1=[]\n",
    "#for i, cluster in enumerate(clustered_sentences):\n",
    "   # prediction = prediction.append(pd.DataFrame(data={'i': i, 'cluster': cluster}, index = [0]), ignore_index = True)\n",
    "#    print(\"Cluster \", i+1)\n",
    "#    print(cluster)\n",
    "#    print(\"\")\n",
    "  #  prediction = prediction.append(pd.DataFrame(data={'i': i, 'cluster': cluster}, index = [0]), ignore_index = True)\n",
    "cluster1 = [cluster for cluster in enumerate(clustered_sentences)]\n",
    "cluster1 = [tup[1] for tup in cluster1]\n",
    "data1 = {'cluster': cluster1}\n",
    "for index in range(len(data1)):\n",
    "    dff= pd.DataFrame(data1, columns=['cluster'])\n",
    "    dff.index.names = ['Cluster ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d429fbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dff.to_csv(\"C:\\\\Users\\\\KD129201\\\\try.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98eba2c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
